{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826598a7-2e58-4140-9e9d-fe4134478b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas sentencepiece tokenizers transformers protobuf datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8ee52-0df8-478b-a807-f14279644875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure tokenizers is up to date (0.15.0+) otherwise install/update\n",
    "# pip install -U tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17269c5-fedf-4501-9486-a35b8e4a8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655db323-079c-4529-aa04-1898e3bcffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers version: 0.21.1\n",
      "SentencePieceUnigramTokenizer.from_file() is not available.\n",
      "available 'from_...' methods: ['from_spm']\n",
      "Tokenizer.from_file() is available.\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "print(f\"Tokenizers version: {tokenizers.__version__}\")\n",
    "\n",
    "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
    "\n",
    "# checking if there is a from_file method\n",
    "if hasattr(SentencePieceUnigramTokenizer, 'from_file'):\n",
    "    print(\"SentencePieceUnigramTokenizer.from_file() is available.\")\n",
    "else:\n",
    "    print(\"SentencePieceUnigramTokenizer.from_file() is not available.\")\n",
    "    # if it is not present getting available methods starting with \"from_\"\n",
    "    print(\"available 'from_...' methods:\", [m for m in dir(SentencePieceUnigramTokenizer) if m.startswith('from_')])\n",
    "\n",
    "# also check the base Tokenizer class maybe from_file there\n",
    "from tokenizers import Tokenizer\n",
    "if hasattr(Tokenizer, 'from_file'):\n",
    "    print(\"Tokenizer.from_file() is available.\")\n",
    "else:\n",
    "    print(\"Tokenizer.from_file() is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd2af3-fb4f-4059-a68d-461b108318be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from tokenizers import SentencePieceUnigramTokenizer # for .from_spm()\n",
    "from tokenizers import Tokenizer # for Tokenizer.from_file() if needed (not here)\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import tempfile\n",
    "import traceback # for more detailed error output\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_NAME = \"LocalDoc/AzTC\"\n",
    "TEXT_COLUMN = \"text\"\n",
    "\n",
    "# number of rows to train the tokenizer\n",
    "# If False use the entire dataset\n",
    "# If a number use the specified number of rows\n",
    "NUM_TRAINING_SAMPLES = 10000000\n",
    "\n",
    "TEMP_TRAIN_DATA_FILE_PREFIX = \"spm_train_data_\"\n",
    "SPM_MODEL_PREFIX = \"aztc_azerbaijani_spm\"\n",
    "VOCAB_SIZE = 32000\n",
    "CHARACTER_COVERAGE = 0.9995\n",
    "MODEL_TYPE = \"unigram\"\n",
    "\n",
    "# strings we want to use for special tokens\n",
    "UNK_PIECE_STR = \"[UNK]\"\n",
    "PAD_PIECE_STR = \"[PAD]\"\n",
    "BOS_PIECE_STR = \"[CLS]\" # using CLS as BOS\n",
    "EOS_PIECE_STR = \"[SEP]\" # using SEP like EOS\n",
    "MASK_PIECE_STR = \"[MASK]\" # this is our \"extra\" special token\n",
    "\n",
    "# tokens that we explicitly want to add to the dictionary via user_defined_symbols,\n",
    "# and that are NOT the default unk/pad/bos/eos for SentencePiece.\n",
    "# SentencePiece will figure out unk/pad/bos/eos via _piece parameters.\n",
    "SPM_USER_DEFINED_SYMBOLS_ONLY_CUSTOM = MASK_PIECE_STR # Only MASK here\n",
    "\n",
    "HF_TOKENIZER_OUTPUT_DIR = \"./aztc_tokenizer_hf\"\n",
    "\n",
    "temp_train_data_path = None\n",
    "\n",
    "try:\n",
    "    # --- 0. Check tokenizers version and method availability (for debugging) ---\n",
    "    print(\"--- Tokenizers library information ---\")\n",
    "    import tokenizers as tk_lib # use alias to avoid conflict with variable\n",
    "    print(f\"Tokenizers version: {tk_lib.__version__}\")\n",
    "    if hasattr(SentencePieceUnigramTokenizer, 'from_spm'):\n",
    "        print(\"SentencePieceUnigramTokenizer.from_spm() is available.\")\n",
    "    else:\n",
    "        print(\"SentencePieceUnigramTokenizer.from_spm() is not available. this could be a problem.\")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    # --- 1.Load AzTC dataset ---\n",
    "    print(f\"Loading dataset {DATASET_NAME}...\")\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    \n",
    "    # get train split (usually the main split for such datasets)\n",
    "    if 'train' in dataset:\n",
    "        train_data = dataset['train']\n",
    "    else:\n",
    "        # If no train split, take the first available one\n",
    "        available_splits = list(dataset.keys())\n",
    "        print(f\"Available splits: {available_splits}\")\n",
    "        train_data = dataset[available_splits[0]]\n",
    "    \n",
    "    print(f\"Dataset loaded. Total number of samples: {len(train_data)}\")\n",
    "    \n",
    "    if TEXT_COLUMN not in train_data.column_names:\n",
    "        raise ValueError(f\"Column '{TEXT_COLUMN}' not found in dataset. Available columns: {train_data.column_names}\")\n",
    "\n",
    "    # --- 2. Prepare training data ---\n",
    "    print(\"Preparing texts for training...\")\n",
    "    \n",
    "    if NUM_TRAINING_SAMPLES is False:\n",
    "        print(\"Using entire dataset for tokenizer training\")\n",
    "        texts_to_use = train_data[TEXT_COLUMN]\n",
    "        samples_count = len(train_data)\n",
    "    else:\n",
    "        print(f\"Using {NUM_TRAINING_SAMPLES} samples for tokenizer training\")\n",
    "        samples_count = min(NUM_TRAINING_SAMPLES, len(train_data))\n",
    "        # Take first N samples\n",
    "        texts_to_use = train_data.select(range(samples_count))[TEXT_COLUMN]\n",
    "    \n",
    "    print(f\"Number of texts for training: {samples_count}\")\n",
    "\n",
    "    # Create temporary file for SentencePiece training\n",
    "    fd, temp_train_data_path = tempfile.mkstemp(prefix=TEMP_TRAIN_DATA_FILE_PREFIX, suffix=\".txt\")\n",
    "    with os.fdopen(fd, \"w\", encoding=\"utf-8\") as tmp_file:\n",
    "        for text in texts_to_use:\n",
    "            if text and isinstance(text, str) and text.strip(): #  check that text is not empty\n",
    "                tmp_file.write(text.strip() + \"\\n\")\n",
    "    \n",
    "    print(f\"Texts written to temporary file: {temp_train_data_path}\")\n",
    "\n",
    "    # --- 3. Train SentencePiece tokenizer ---\n",
    "    print(\"\\nTraining SentencePiece tokenizer...\")\n",
    "\n",
    "    spm_command_args = [\n",
    "        f\"--input={temp_train_data_path}\",\n",
    "        f\"--model_prefix={SPM_MODEL_PREFIX}\",\n",
    "        f\"--vocab_size={VOCAB_SIZE}\",\n",
    "        f\"--character_coverage={CHARACTER_COVERAGE}\",\n",
    "        f\"--model_type={MODEL_TYPE}\",\n",
    "        f\"--unk_piece={UNK_PIECE_STR}\",\n",
    "        f\"--pad_piece={PAD_PIECE_STR}\",\n",
    "        f\"--bos_piece={BOS_PIECE_STR}\",\n",
    "        f\"--eos_piece={EOS_PIECE_STR}\",\n",
    "        f\"--user_defined_symbols={SPM_USER_DEFINED_SYMBOLS_ONLY_CUSTOM}\",\n",
    "        \"--shuffle_input_sentence=true\",\n",
    "        f\"--input_sentence_size={min(20000000, samples_count)}\", # limit for speed if needed\n",
    "        \"--hard_vocab_limit=false\",\n",
    "    ]\n",
    "    \n",
    "    spm_command_str = \" \".join(spm_command_args)\n",
    "    print(f\"SPM command: {spm_command_str}\")\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(spm_command_str)\n",
    "    print(f\"SentencePiece model trained and saved with prefix: {SPM_MODEL_PREFIX}\")\n",
    "    spm_model_file = f\"{SPM_MODEL_PREFIX}.model\"\n",
    "    if not os.path.exists(spm_model_file):\n",
    "        raise FileNotFoundError(f\"SentencePiece model file {spm_model_file} not found after training.\")\n",
    "\n",
    "    # --- 4. Convert to Hugging Face tokenizer ---\n",
    "    print(\"\\nConverting to Hugging Face tokenizer...\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Trying via SentencePieceUnigramTokenizer.from_spm('{spm_model_file}')...\")\n",
    "        hf_tokenizer_slow = SentencePieceUnigramTokenizer.from_spm(spm_model_file)\n",
    "        print(\"Loaded using SentencePieceUnigramTokenizer.from_spm()\")\n",
    "\n",
    "    except Exception as e_load: # use diferent name for exception variable\n",
    "        print(f\"Error loading via SentencePieceUnigramTokenizer.from_spm(): {e_load}\")\n",
    "        print(\"If this is AttributeError, make sure the 'tokenizers' version is fresh enough (0.15.0+).\")\n",
    "        traceback.print_exc() \n",
    "        raise # re-raise error to stop execution if loading failed\n",
    "\n",
    "    hf_tokenizer_fast = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=hf_tokenizer_slow,\n",
    "        unk_token=UNK_PIECE_STR,\n",
    "        pad_token=PAD_PIECE_STR,\n",
    "        cls_token=BOS_PIECE_STR, \n",
    "        sep_token=EOS_PIECE_STR, \n",
    "        mask_token=MASK_PIECE_STR,\n",
    "        # additional parameters if needed for our model:\n",
    "        # model_max_length=512, # if we need to limit default length\n",
    "        # padding_side='right', # or 'left'\n",
    "        # truncation_side='right', # or 'left'\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(HF_TOKENIZER_OUTPUT_DIR):\n",
    "        os.makedirs(HF_TOKENIZER_OUTPUT_DIR)\n",
    "\n",
    "    hf_tokenizer_fast.save_pretrained(HF_TOKENIZER_OUTPUT_DIR)\n",
    "    print(f\"Hugging Face tokenizer saved to: {HF_TOKENIZER_OUTPUT_DIR}\")\n",
    "\n",
    "    # --- 5. Testing ---\n",
    "    print(\"\\n--- Testing tokenizer ---\")\n",
    "    # load saved hf tokenizer for verification\n",
    "    tokenizer_test = PreTrainedTokenizerFast.from_pretrained(HF_TOKENIZER_OUTPUT_DIR)\n",
    "\n",
    "    test_sentences = [\n",
    "        \"Bu Azərbaycan dilində bir cümlədir.\",\n",
    "        \"Azərbaycan Respublikasının paytaxtı Bakı şəhəridir.\",\n",
    "    ]\n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        encoded = tokenizer_test.encode(sentence)\n",
    "        tokens = tokenizer_test.convert_ids_to_tokens(encoded)\n",
    "        print(f\"\\nOriginal: {sentence}\")\n",
    "        print(f\"Encoded IDs: {encoded}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        # print(f\"decoded: {tokenizer_test.decode(encoded)}\") # decode can behave differently with special tokens\n",
    "\n",
    "    print(f\"\\ntokenizer vocabulary contains {tokenizer_test.vocab_size} tokens.\")\n",
    "    print(f\"PAD token: '{tokenizer_test.pad_token}' (ID: {tokenizer_test.pad_token_id})\")\n",
    "    print(f\"UNK token: '{tokenizer_test.unk_token}' (ID: {tokenizer_test.unk_token_id})\")\n",
    "    print(f\"CLS token: '{tokenizer_test.cls_token}' (ID: {tokenizer_test.cls_token_id})\")\n",
    "    print(f\"SEP token: '{tokenizer_test.sep_token}' (ID: {tokenizer_test.sep_token_id})\")\n",
    "    print(f\"MASK token: '{tokenizer_test.mask_token}' (ID: {tokenizer_test.mask_token_id})\")\n",
    "\n",
    "    # Show examples from original dataset\n",
    "    print(f\"\\n--- Examples from AzTC dataset ---\")\n",
    "    sample_texts = train_data.select(range(min(5, len(train_data))))[TEXT_COLUMN]\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        print(f\"Example {i+1}: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "\n",
    "except Exception as e_main: # name for general exception\n",
    "    print(f\"\\nMain script execution error occurred: {e_main}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    if temp_train_data_path and os.path.exists(temp_train_data_path):\n",
    "        print(f\"\\nRemoving temporary file: {temp_train_data_path}\")\n",
    "        os.remove(temp_train_data_path)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54dcb169-23ad-4a03-ac21-bd53b08655b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6248e67dc14f9193e96a8c6b3ee2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea21abf4dcc642b38ee101a429337a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cea0e183ec4436fa55d730750371c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85293e0d8aa543f9b88d69b5c1adb59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de4f08b2b6f4462b04a2e94a6219670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a1a866361a49e9bc65c8ed6860761f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e507b2b973e1473bbedc36d5de108a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eebfc049bea4fb8b5a4ab0947d21fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 1000 samples...\n",
      "\n",
      "=== SPEED COMPARISON ===\n",
      "Local: 0.0174 seconds\n",
      "XLM-RoBERTa: 0.0182 seconds\n",
      "mBERT: 0.0428 seconds\n",
      "\n",
      "=== QUALITY ANALYSIS ===\n",
      "             avg_tokens  avg_unk_tokens  unk_percentage  vocab_size\n",
      "Local             37.85            0.01            0.02     32000.0\n",
      "XLM-RoBERTa       50.15            0.00            0.00    250002.0\n",
      "mBERT             64.46            1.14            1.77    119547.0\n",
      "\n",
      "=== VOCABULARY EFFICIENCY ===\n",
      "Local: 845.38 (vocab_size/avg_tokens)\n",
      "XLM-RoBERTa: 4985.48 (vocab_size/avg_tokens)\n",
      "mBERT: 1854.45 (vocab_size/avg_tokens)\n",
      "=== DETAILED TOKENIZATION COMPARISON ===\n",
      "\n",
      "Sample 1: — Aşağıdakı hallarda işlədilən durğu işarəsi: Nida cümləsinin sonunda....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (14): ['▁—', '▁Aşağıdakı', '▁hallarda', '▁işlədilən', '▁dur', 'ğu', '▁işarəsi', ':', '▁Ni', 'da', '▁cümləsi', 'nin', '▁sonunda', '.']\n",
      "  Token IDs: [46, 22805, 1354, 21606, 3765, 5169, 16966, 25, 2162, 21, 26966, 37, 1493, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (22): ['<s>', '▁—', '▁Aşağı', 'dakı', '▁hallarda', '▁işlə', 'd', 'ilən', '▁dur', 'ğu', '▁işarə', 'si', ':', '▁Ni', 'da', '▁', 'cü', 'ml', 'əsinin', '▁sonunda', '.', '</s>']\n",
      "  Token IDs: [0, 292, 140051, 23796, 127374, 59114, 71, 57369, 7920, 29563, 231393, 172, 12, 1520, 85, 6, 8893, 7237, 119975, 53739, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (25): ['[CLS]', '[UNK]', 'A', '##şağı', '##dakı', 'hall', '##arda', 'iş', '##lə', '##dil', '##ən', 'dur', '##ğu', 'iş', '##ar', '##əsi', ':', 'Ni', '##da', 'cü', '##ml', '##əsinin', 'sonunda', '.', '[SEP]']\n",
      "  Token IDs: [101, 100, 138, 96754, 80457, 21007, 44634, 50584, 19277, 28113, 16931, 28959, 31747, 50584, 10354, 17871, 131, 30409, 10229, 18358, 63308, 37180, 44976, 119, 102]\n",
      "\n",
      "Sample 2: Məsələn: Azərbaycan dilində /Yanğın!/, /Fəlakət!/; əmr cümlələrində /Rədd ol burdan!/; Çağırış və mü...\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (36): ['▁Məsələn', ':', '▁Azərbaycan', '▁dilində', '▁/', 'Yan', 'ğın', '!', '/', ',', '▁/', 'F', 'ə', 'la', 'kət', '!', '/', ';', '▁əmr', '▁cümlə', 'lərində', '▁/', 'R', 'ə', 'dd', '▁ol', '▁burdan', '!', '/', ';', '▁Çağırış', '▁və', '▁müraciət', '▁həyəcanlı', '▁olanda', '.']\n",
      "  Token IDs: [913, 25, 39, 990, 2042, 15412, 4333, 658, 374, 5, 2042, 427, 23, 52, 23361, 658, 374, 115, 2598, 7018, 507, 2042, 387, 23, 7283, 5223, 17850, 658, 374, 115, 23141, 6, 346, 26716, 3174, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (47): ['<s>', '▁Məsələn', ':', '▁Azərbaycan', '▁dilində', '▁/', 'Yan', 'ğın', '!', '/', ',', '▁/', 'F', 'ə', 'lak', 'ət', '!', '/', ';', '▁əmr', '▁c', 'üm', 'lə', 'lərində', '▁/', 'R', 'ə', 'dd', '▁ol', '▁bur', 'dan', '!', '/', ';', '▁Çağ', 'ır', 'ış', '▁və', '▁müraciət', '▁h', 'əyə', 'can', 'lı', '▁olan', 'da', '.', '</s>']\n",
      "  Token IDs: [0, 142164, 12, 2204, 126949, 248, 72306, 60233, 38, 64, 4, 248, 919, 1354, 4478, 9485, 38, 64, 74, 157377, 501, 16588, 3855, 19547, 248, 1052, 1354, 4028, 4998, 6818, 549, 38, 64, 74, 126793, 3772, 13827, 530, 36120, 1096, 40875, 4398, 1304, 1425, 85, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (53): ['[CLS]', 'M', '##əsələn', ':', 'Azərbaycan', 'dilində', '/', 'Yan', '##ğın', '!', '/', ',', '/', 'F', '##əl', '##ak', '##ət', '!', '/', ';', 'ə', '##m', '##r', 'cü', '##ml', '##ələrində', '/', 'R', '##əd', '##d', 'ol', 'bu', '##rdan', '!', '/', ';', 'Ç', '##ağı', '##rı', '##ş', 'və', 'm', '##üra', '##ci', '##ət', 'h', '##əyə', '##can', '##lı', 'olan', '##da', '.', '[SEP]']\n",
      "  Token IDs: [101, 150, 109198, 131, 13556, 80713, 120, 44660, 61719, 106, 120, 117, 120, 143, 79598, 10710, 17393, 106, 120, 132, 391, 10147, 10129, 18358, 63308, 101861, 120, 155, 55866, 10162, 30668, 11499, 75252, 106, 120, 132, 232, 92358, 18729, 12843, 10711, 181, 82505, 10598, 17393, 176, 51674, 24154, 12407, 11753, 10229, 119, 102]\n",
      "\n",
      "Sample 3: Məsələn: Azərbaycan dilində /Yaşasın müstəqil Azərbaycan!//; Nida cümlələrində özəksonu zəifləyir, z...\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (28): ['▁Məsələn', ':', '▁Azərbaycan', '▁dilində', '▁/', 'Yaşa', 'sın', '▁müstəqil', '▁Azərbaycan', '!', '/', '/', ';', '▁Ni', 'da', '▁cümlə', 'lərində', '▁öz', 'ək', 'son', 'u', '▁zəifləyir', ',', '▁zaman', '▁ləng', 'i', 'yir', '.']\n",
      "  Token IDs: [913, 25, 39, 990, 2042, 20038, 1562, 1039, 39, 658, 374, 374, 115, 2162, 21, 7018, 507, 82, 1791, 936, 89, 28448, 5, 192, 10339, 16, 5184, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (34): ['<s>', '▁Məsələn', ':', '▁Azərbaycan', '▁dilində', '▁/', 'Ya', 'şa', 'sın', '▁müstəqil', '▁Azərbaycan', '!', '//', ';', '▁Ni', 'da', '▁c', 'üm', 'lə', 'lərində', '▁öz', 'ək', 'son', 'u', '▁zəif', 'ləyir', ',', '▁zaman', '▁', 'lən', 'gi', 'yir', '.', '</s>']\n",
      "  Token IDs: [0, 142164, 12, 2204, 126949, 248, 21566, 12102, 24061, 107278, 2204, 38, 20767, 74, 1520, 85, 501, 16588, 3855, 19547, 6692, 22393, 1681, 34, 184239, 101321, 4, 3859, 6, 24703, 735, 62610, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (43): ['[CLS]', 'M', '##əsələn', ':', 'Azərbaycan', 'dilində', '/', 'Ya', '##şa', '##sı', '##n', 'm', '##üst', '##ə', '##qi', '##l', 'Azərbaycan', '!', '/', '/', ';', 'Ni', '##da', 'cü', '##ml', '##ələrində', 'öz', '##ək', '##son', '##u', 'z', '##ə', '##if', '##lə', '##yir', ',', 'zaman', 'l', '##ən', '##gi', '##yir', '.', '[SEP]']\n",
      "  Token IDs: [101, 150, 109198, 131, 13556, 80713, 120, 25148, 38959, 13836, 10115, 181, 61785, 11562, 27132, 10161, 13556, 106, 120, 120, 132, 30409, 10229, 18358, 63308, 101861, 25898, 28708, 11599, 10138, 194, 11562, 13918, 19277, 38708, 117, 19130, 180, 16931, 11210, 38708, 119, 102]\n",
      "\n",
      "Sample 4: O, müqəddəs bir kainatdır//....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (9): ['▁O', ',', '▁müqəddəs', '▁bir', '▁kainat', 'dır', '/', '/', '.']\n",
      "  Token IDs: [71, 5, 2738, 11, 17552, 65, 374, 374, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (11): ['<s>', '▁O', ',', '▁müqəddəs', '▁bir', '▁kaina', 't', 'dır', '//', '.', '</s>']\n",
      "  Token IDs: [0, 180, 4, 235280, 263, 38981, 18, 2544, 20767, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (17): ['[CLS]', 'O', ',', 'm', '##ü', '##q', '##əd', '##də', '##s', 'bir', 'kai', '##nat', '##dır', '/', '/', '.', '[SEP]']\n",
      "  Token IDs: [101, 152, 117, 181, 12369, 11703, 55866, 12448, 10107, 10561, 27803, 20909, 14110, 120, 120, 119, 102]\n",
      "\n",
      "Sample 5: — \"Toxunulmazlar\") 2011-ci ildə real hadisələrə əsaslanan fransız komediya dramı müvəffəqiyyətli ari...\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (25): ['▁—', '▁\"', 'T', 'oxu', 'n', 'ul', 'maz', 'lar', '\")', '▁2011-', 'ci', '▁ildə', '▁real', '▁hadisələrə', '▁əsaslanan', '▁fransız', '▁komediya', '▁dram', 'ı', '▁müvəffəqiyyət', 'li', '▁aristokrat', '▁Filipp', '▁haqqında', '.']\n",
      "  Token IDs: [46, 27, 403, 16710, 18, 612, 3322, 28, 2665, 1207, 22, 33, 1146, 9784, 6419, 2136, 9134, 5919, 68, 9217, 59, 24244, 17555, 177, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (37): ['<s>', '▁—', '▁\"', 'To', 'xun', 'u', 'lmaz', 'lar', '\")', '▁2011-', 'ci', '▁ildə', '▁real', '▁hadisələr', 'ə', '▁əsas', 'lanan', '▁fran', 'sız', '▁komedi', 'ya', '▁dram', 'ı', '▁mü', 'və', 'ff', 'ə', 'q', 'iyyət', 'li', '▁ar', 'isto', 'krat', '▁Filipp', '▁haqqında', '.', '</s>']\n",
      "  Token IDs: [0, 292, 44, 7763, 65608, 34, 31575, 320, 18939, 79279, 318, 11607, 2773, 94269, 1354, 15021, 18850, 27333, 10782, 60471, 395, 46494, 1057, 8626, 35657, 4902, 1354, 864, 23161, 150, 187, 5176, 15207, 132989, 10539, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (44): ['[CLS]', '[UNK]', '\"', 'To', '##xu', '##nul', '##maz', '##lar', '\"', ')', '2011', '-', 'ci', 'ildə', 'real', 'hadi', '##sə', '##lərə', 'əsas', '##lanan', 'fra', '##ns', '##ız', 'komedi', '##ya', 'dra', '##mı', 'm', '##ü', '##v', '##ə', '##ff', '##ə', '##qiy', '##yət', '##li', 'ari', '##sto', '##krat', 'Filip', '##p', 'haqqında', '.', '[SEP]']\n",
      "  Token IDs: [101, 100, 107, 11469, 39234, 41464, 39125, 10774, 107, 114, 10158, 118, 11322, 12771, 13486, 52661, 52639, 45917, 36732, 31395, 10628, 10891, 30471, 76886, 10679, 68507, 36544, 181, 12369, 10477, 11562, 13820, 11562, 82001, 109337, 10390, 57173, 15340, 31604, 31445, 10410, 30588, 119, 102]\n",
      "\n",
      "=== AZERBAIJANI SPECIFIC TESTS ===\n",
      "=== DETAILED TOKENIZATION COMPARISON ===\n",
      "\n",
      "Sample 1: Azərbaycan Respublikası Cənubi Qafqazda yerləşən ölkədir....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (7): ['▁Azərbaycan', '▁Respublikası', '▁Cənubi', '▁Qafqazda', '▁yerləşən', '▁ölkədir', '.']\n",
      "  Token IDs: [39, 410, 742, 2810, 365, 9923, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (11): ['<s>', '▁Azərbaycan', '▁Respublikası', '▁Cənubi', '▁Qafqaz', 'da', '▁yerləşən', '▁ölkə', 'dir', '.', '</s>']\n",
      "  Token IDs: [0, 2204, 22915, 152016, 97448, 85, 80663, 27707, 936, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (13): ['[CLS]', 'Azərbaycan', 'Respublikası', 'Cənubi', 'Qafqaz', '##da', 'yerləşən', 'ö', '##lk', '##əd', '##ir', '.', '[SEP]']\n",
      "  Token IDs: [101, 13556, 33510, 83945, 94771, 10229, 46696, 276, 24323, 55866, 10835, 119, 102]\n",
      "\n",
      "Sample 2: Bakı şəhəri Azərbaycanın paytaxtı və ən böyük şəhəridir....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (9): ['▁Bakı', '▁şəhəri', '▁Azərbaycanın', '▁paytaxtı', '▁və', '▁ən', '▁böyük', '▁şəhəridir', '.']\n",
      "  Token IDs: [129, 776, 166, 1713, 6, 84, 97, 9551, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (12): ['<s>', '▁Bakı', '▁şəhəri', '▁Azərbaycanın', '▁paytaxtı', '▁və', '▁ən', '▁böyük', '▁şəhəri', 'dir', '.', '</s>']\n",
      "  Token IDs: [0, 9741, 56832, 16540, 163536, 530, 9625, 12160, 56832, 936, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (14): ['[CLS]', 'Bakı', 'şəhəri', 'Azərbaycanın', 'pay', '##tax', '##tı', 'və', 'ən', 'böyük', 'şəhəri', '##dir', '.', '[SEP]']\n",
      "  Token IDs: [101, 17458, 75988, 41042, 16868, 90188, 19385, 10711, 22770, 25093, 75988, 11957, 119, 102]\n",
      "\n",
      "Sample 3: Xəzər dənizi Azərbaycanın şərq sərhədini təşkil edir....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (8): ['▁Xəzər', '▁dənizi', '▁Azərbaycanın', '▁şərq', '▁sərhədini', '▁təşkil', '▁edir', '.']\n",
      "  Token IDs: [1228, 3197, 166, 2375, 21055, 180, 63, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (13): ['<s>', '▁Xəzər', '▁dəniz', 'i', '▁Azərbaycanın', '▁', 'şərq', '▁sərhəd', 'ini', '▁təşkil', '▁edir', '.', '</s>']\n",
      "  Token IDs: [0, 130694, 108275, 14, 16540, 6, 198034, 118050, 943, 20916, 7284, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (20): ['[CLS]', 'X', '##ə', '##zə', '##r', 'dəniz', '##i', 'Azərbaycanın', 'ş', '##ər', '##q', 's', '##ər', '##h', '##əd', '##ini', 'təşkil', 'edir', '.', '[SEP]']\n",
      "  Token IDs: [101, 161, 11562, 43588, 10129, 102616, 10116, 41042, 350, 16540, 11703, 187, 16540, 10237, 55866, 11778, 32828, 19559, 119, 102]\n",
      "\n",
      "Sample 4: Azərbaycan dilində 32 hərf var və latın əlifbasından istifadə olunur....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (12): ['▁Azərbaycan', '▁dilində', '▁32', '▁hərf', '▁var', '▁və', '▁latın', '▁əlifbası', 'ndan', '▁istifadə', '▁olunur', '.']\n",
      "  Token IDs: [39, 990, 3419, 11244, 43, 6, 8424, 8493, 150, 117, 185, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (18): ['<s>', '▁Azərbaycan', '▁dilində', '▁32', '▁hər', 'f', '▁var', '▁və', '▁l', 'atın', '▁əl', 'if', 'ba', 'sından', '▁istifadə', '▁olunur', '.', '</s>']\n",
      "  Token IDs: [0, 2204, 126949, 2789, 7580, 420, 285, 530, 96, 74387, 34747, 3190, 402, 55919, 11466, 25329, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (18): ['[CLS]', 'Azərbaycan', 'dilində', '32', 'hər', '##f', 'var', 'və', 'lat', '##ın', 'əl', '##if', '##bas', '##ından', 'istifadə', 'olunur', '.', '[SEP]']\n",
      "  Token IDs: [101, 13556, 80713, 10842, 33205, 10575, 10299, 10711, 12764, 12216, 87449, 13918, 21322, 39201, 27639, 38186, 119, 102]\n",
      "\n",
      "Sample 5: Naxçıvan Azərbaycanın muxtar respublikasıdır....\n",
      "--------------------------------------------------\n",
      "Local:\n",
      "  Tokens (6): ['▁Naxçıvan', '▁Azərbaycanın', '▁muxtar', '▁respublikası', 'dır', '.']\n",
      "  Token IDs: [800, 166, 8523, 15813, 65, 4]\n",
      "XLM-RoBERTa:\n",
      "  Tokens (9): ['<s>', '▁Naxçıvan', '▁Azərbaycanın', '▁mux', 'tar', '▁respublika', 'sıdır', '.', '</s>']\n",
      "  Token IDs: [0, 67128, 16540, 113475, 867, 53100, 123809, 5, 2]\n",
      "mBERT:\n",
      "  Tokens (12): ['[CLS]', 'Naxçıvan', 'Azərbaycanın', 'mu', '##xta', '##r', 'resp', '##ublik', '##ası', '##dır', '.', '[SEP]']\n",
      "  Token IDs: [101, 60116, 41042, 12361, 89414, 10129, 52812, 77186, 26873, 14110, 119, 102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizerFast, XLMRobertaTokenizer, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "LOCAL_TOKENIZER_PATH = \"./aztc_tokenizer_hf\"\n",
    "DATASET_NAME = \"LocalDoc/AzTC\"\n",
    "TEXT_COLUMN = \"text\"\n",
    "NUM_TEST_SAMPLES = 1000\n",
    "\n",
    "def load_tokenizers():\n",
    "    local_tokenizer = PreTrainedTokenizerFast.from_pretrained(LOCAL_TOKENIZER_PATH)\n",
    "    xlm_roberta_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    \n",
    "    return {\n",
    "        \"Local\": local_tokenizer,\n",
    "        \"XLM-RoBERTa\": xlm_roberta_tokenizer,\n",
    "        \"mBERT\": mbert_tokenizer\n",
    "    }\n",
    "\n",
    "def test_tokenization_speed(tokenizers, texts):\n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        start_time = time.time()\n",
    "        for text in texts:\n",
    "            tokenizer.encode(text, add_special_tokens=True)\n",
    "        end_time = time.time()\n",
    "        results[name] = end_time - start_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_tokenization_quality(tokenizers, texts):\n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        token_counts = []\n",
    "        unk_counts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "            token_counts.append(len(tokens))\n",
    "            \n",
    "            token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            unk_count = sum(1 for token in token_strings if token in [tokenizer.unk_token, \"<unk>\", \"[UNK]\"])\n",
    "            unk_counts.append(unk_count)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"avg_tokens\": np.mean(token_counts),\n",
    "            \"avg_unk_tokens\": np.mean(unk_counts),\n",
    "            \"unk_percentage\": (np.sum(unk_counts) / np.sum(token_counts)) * 100,\n",
    "            \"vocab_size\": tokenizer.vocab_size\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def detailed_comparison(tokenizers, sample_texts):\n",
    "    print(\"=== DETAILED TOKENIZATION COMPARISON ===\\n\")\n",
    "    \n",
    "    for i, text in enumerate(sample_texts[:5]):\n",
    "        print(f\"Sample {i+1}: {text[:100]}...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name, tokenizer in tokenizers.items():\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "            token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "            \n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  Tokens ({len(tokens)}): {token_strings}\")\n",
    "            print(f\"  Token IDs: {tokens}\")\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    print(\"Loading tokenizers...\")\n",
    "    tokenizers = load_tokenizers()\n",
    "    \n",
    "    print(\"Loading test dataset...\")\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    if 'train' in dataset:\n",
    "        test_data = dataset['train']\n",
    "    else:\n",
    "        test_data = dataset[list(dataset.keys())[0]]\n",
    "    \n",
    "    test_texts = test_data.select(range(min(NUM_TEST_SAMPLES, len(test_data))))[TEXT_COLUMN]\n",
    "    test_texts = [text for text in test_texts if text and isinstance(text, str) and text.strip()]\n",
    "    \n",
    "    print(f\"Testing with {len(test_texts)} samples...\")\n",
    "    \n",
    "    print(\"\\n=== SPEED COMPARISON ===\")\n",
    "    speed_results = test_tokenization_speed(tokenizers, test_texts[:100])\n",
    "    for name, time_taken in speed_results.items():\n",
    "        print(f\"{name}: {time_taken:.4f} seconds\")\n",
    "    \n",
    "    print(\"\\n=== QUALITY ANALYSIS ===\")\n",
    "    quality_results = analyze_tokenization_quality(tokenizers, test_texts)\n",
    "    \n",
    "    df = pd.DataFrame(quality_results).T\n",
    "    print(df.round(2))\n",
    "    \n",
    "    print(\"\\n=== VOCABULARY EFFICIENCY ===\")\n",
    "    for name, results in quality_results.items():\n",
    "        efficiency = results['vocab_size'] / results['avg_tokens']\n",
    "        print(f\"{name}: {efficiency:.2f} (vocab_size/avg_tokens)\")\n",
    "    \n",
    "    detailed_comparison(tokenizers, test_texts)\n",
    "    \n",
    "    print(\"=== AZERBAIJANI SPECIFIC TESTS ===\")\n",
    "    azerbaijani_samples = [\n",
    "        \"Azərbaycan Respublikası Cənubi Qafqazda yerləşən ölkədir.\",\n",
    "        \"Bakı şəhəri Azərbaycanın paytaxtı və ən böyük şəhəridir.\",\n",
    "        \"Xəzər dənizi Azərbaycanın şərq sərhədini təşkil edir.\",\n",
    "        \"Azərbaycan dilində 32 hərf var və latın əlifbasından istifadə olunur.\",\n",
    "        \"Naxçıvan Azərbaycanın muxtar respublikasıdır.\"\n",
    "    ]\n",
    "    \n",
    "    detailed_comparison(tokenizers, azerbaijani_samples)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4c024-5daa-40de-8422-70db676aba1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
